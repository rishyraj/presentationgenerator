Today, the world is more connected than ever, due to advancements in digital technology. Alongside this advancement is advancement in encryption and security. Digital technology used to not have any form of encryption. Nowadays, almost every single person today uses some form of encryption to access their digital belongings including their laptops and smartphones. This form of authentication can include passcodes, auto-generated keys, face unlock, fingerprint authentication, or even a combination of any of these measures known as multi-factor authentication. We do this because we don’t want strangers to view our private information stored on our devices. However, due to rising tensions between governments resulting in terrorism and other threats to national security, governments all over the world are advocating for backdoor access into popular softwares including the Windows OS, Macintosch OS, IoS, messaging services etc. According to Rich Harity of New Atlas, terrorists use encrypted messaging services, such as WhatsApp to communicate and plan out terrorism events, such as the Westminster Bridge terrorism event (2017). During that event British Authorities wanted WhatsApp to give them backdoor access, but WhatsApp said no, since they themselves can’t access the data. This has been the case for multiple software now. While backdoor access for governments sounds like a reasonable thing to ask in our day and age, backdoor access to any software will cause more harm than good, since it will endanger the digital security of every single person.
First, a definition of backdoor access needs to be established in order to proceed with the case against backdoor access for governments. MalwareBytes states that “[backdoor access] refers to any method by which authorized and unauthorized users are able to get around normal security measures and gain high level user access (aka root access) on a computer system, network, or software application.” (2019). To take this into a different context, imagine a burglar wants to rob a house, but he sees that the front door is locked with a lock that can’t be picked easily. Instead, he goes to the back of the house by hopping the fence and sees a door that has no lock. The burglar can proceed to rob the house and escape through the backdoor without triggering any alarms or causing any damage. In fact, if the burglar is smart, he would only touch the things that matter towards his end goal, and not touch anything else. That way, he can keep using the backdoor of the house without raising the suspicions of the home owners. Now taking this into a digital context, a hacker can use a backdoor to read sensitive documents, install malware, or even hijack the device for multiple purposes. It is also important to differentiate backdoor access from other forms of exploits. According to MalwareBytes the main difference is the following: “Exploits are accidental software vulnerabilities used to gain access to your computer and, potentially, deploy some sort of malware. To put it another way, exploits are just software bugs that researchers or cybercriminals have found a way to take advantage of. Backdoors, on the other hand, are deliberately put in place by manufacturers or cybercriminals to get into and out of a system at will.” (2019). This paper will be focusing on backdoors placed by manufacturers specifically for government agencies.
To understand why governments want backdoor access, let’s take a look at the San Bernardino Shooting event in 2017. Law enforcement was able to get the phone of the shooter, and it was an iPhone. Since the phone was an iPhone it was locked with a passcode, and Apple, the manufacturer of the iPhone, is known for having a tightly secure OS with no backdoor access. According to Ian J. McCarthy from the University of Toledo, Apple had helped law enforcement multiple times with various cases ranging from human traffiking to terrorism (2017, p. 179). However Apple had recently implemented a security system that would mean that even Apple employees can’t access or bypass a users device. According to McCarthy, this was a goal that Apple wanted to accomplish for a very long time(2017, p. 180). As a result when Apple was reached out by Law Enforcement, Apple said no. Law enforcement desperately wanted Apple to give them backdoor access so they can build a profile of the shooter and even target other potential suspects for future terrorism events. This started the national debate of software backdoors for the government in the name of national security. Apple was able to win its case, and its future products are even more secure than the shooter’s iPhone.
Law enforcement seems to have a good reason to want backdoor access to software, but why is backdoor access not the solution to their goal? The main reason is that it actually makes their long term goal harder to achieve. The government’s long term goal in this scenario is having a strong national security. However, backdoor access will not only give the host government access to software, but it will also give access to foreign governments as well as cybercriminals. Going back to the house analogy, that unlocked backdoor can not only be accessed by one burglar, but by anyone. Taking this into a national security context, having backdoor access will allow foreign governments to get information about the host government’s citizens that wouldn’t be available to them. The scenarios from this vulnerability are infinite. Foreign governments can cause financial damage, engage in identity theft, engage in driving social behavior or even control a host country’s infrastructure. The last one sounds uncanny, but this happened to multiple US Nuclear Plants in 2017. According to Micheal Riley of Bloomberg, a foreign government was able to use backdoor access to get into 28 Nuclear Powerplant’s control systems, which disrupted a good portion of the US’ energy grid (2017). As one can imagine, disrupting a country’s energy grid is more catastrophic to a country’s national security than not having backdoor access to a terrorist’s phone.
Someone advocating for government backdoors might pose one solution to the above problem, which is having the backdoor access only be accessible to the host government. In other words, if we take the house analogy, that backdoor has a lock that only the host government can access. This sounds like the ideal solution, but this solution is more complicated to implement than it seems. In order to achieve this, a government must have a unique key to every piece of software, otherwise it would be way too easy to crack the government’s backdoors. This now becomes a huge, and perhaps unsolvable, math problem, since there are trillions of copies of multiple software. Creating a secure unique key to access each software becomes super costly, inefficient and most likely impossible today. To understand why, think of encryption as a glove. Encryption protects software as gloves protect someone’s hands. Encryption can be used for nefarious purposes, and gloves can be used to commit crimes without leaving a trace. According to Ronald L. Livest, co-inventor of RSA Encryption, The above solution’s implementation for gloves would involve etching every single person’s fingerprint on the glove and making sure that if a person wears that glove, it must be a glove containing that person’s fingerprint  (1998, p. 117). It’s clear to see from this example that implementation of the above solution is practically impossible. According to Ronald L. Rivest “Strong cryptography only gets easier to implement—and harder to regulate—over time.” (1998, p. 117) The logical contrapositive equivalent of that statement is if strong cryptography is easy to regulate over time, it would be harder to implement.
Requiring software to have backdoor access in the name of national security is a bad idea. Current implementation would in fact weaken national security and a theoretical secure implementation would be impossible to implement. But beyond all of these reasons, a backdoor access makes our private lives not so private anymore. We have privacy in our homes, so why should it not be acceptable to expect the same level of privacy in our digital lives. Privacy and security go hand in hand. If we follow through with UK Home Secretary Amber Rudd’s wishes of “...mak[ing] sure that organizations like WhatsApp, and there are plenty of others like that, don't provide a secret place for terrorists to communicate with each other...” (2017) we are weakening our personal security as well. In other words, asking for backdoor access means taking away your shield from online cyber criminals who don’t care what will happen to you when they exploit you for information. This access can even be exploited for human rights violations. According to Noa Yachot of the ACLU, “...it’s not hard to imagine the Chinese government serving Apple with a warrant to hack into the phone of a dissident activist or intellectual. That development would have a devastating impact on democracy and human rights activists and movements worldwide, which depend on secure communications to flourish.” (2016). This is not a worthwhile cost in the name of national security.